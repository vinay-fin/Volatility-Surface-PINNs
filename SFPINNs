class SFPINN(nn.Module):
    def __init__(self, in_dim, hidden_dim, out_dim, num_layers, sigma=1.0, activation="tanh"):
        super(SFPINN, self).__init__()

        # First sinusoidal mapping
        self.first_linear = nn.Linear(in_dim, hidden_dim, bias=True)
        nn.init.normal_(self.first_linear.weight, mean=0.0, std=sigma)
        nn.init.constant_(self.first_linear.bias, 0.0)

        act_map = {"tanh": nn.Tanh(), "sigmoid": nn.Sigmoid(), "sin":SinAct()}
        self.act = act_map[activation]

        # Hidden layers
        self.hidden_layers = nn.ModuleList()
        for _ in range(num_layers - 2):
            layer = nn.Linear(hidden_dim, hidden_dim)
            nn.init.xavier_normal_(layer.weight)
            nn.init.constant_(layer.bias, 0.0)
            self.hidden_layers.append(layer)

        # Output
        self.output_layer = nn.Linear(hidden_dim, out_dim)

    def forward(self, *inputs):
        src = torch.cat(inputs, dim=-1)
        src = torch.sin(2 * torch.pi * self.first_linear(src))  # sf mapping

        for layer in self.hidden_layers:
            src = self.act(layer(src))

        return self.output_layer(src)
