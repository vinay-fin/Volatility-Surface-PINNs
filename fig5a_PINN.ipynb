{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9284d3e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================\n",
    "# Fig 5(a) for PINNs with TensorFlow/Keras)\n",
    "# =========================\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers, initializers\n",
    "import matplotlib.pyplot as plt\n",
    "import random, math\n",
    "\n",
    "# Use float64 for stable higher-order derivatives\n",
    "tf.keras.backend.set_floatx('float64')\n",
    "\n",
    "# -------- Problem setup --------\n",
    "v = 50.0   # advection (Case v= 1.0 works well)\n",
    "k = 1.0    # diffusion\n",
    "\n",
    "def u_true(x, v=v, k=k):\n",
    "    # analytic solution of k u'' - v u' = 0 with u(0)=0, u(1)=1\n",
    "    return (np.exp((v/k)*x) - 1.0) / (math.e**(v/k) - 1.0)\n",
    "\n",
    "# Dense grid for MSE\n",
    "x_eval = np.linspace(0.0, 1.0, 2001).reshape(-1,1)\n",
    "u_eval = u_true(x_eval)\n",
    "\n",
    "# Collocation + BC points\n",
    "N_c = 2000\n",
    "N_b = 200  # half at x=0, half at x=1\n",
    "# N_c = 100\n",
    "# N_b = 10  # half at x=0, half at x=1\n",
    "\n",
    "rng = np.random.default_rng(123)\n",
    "x_c_np = rng.random((N_c,1))           # uniform in (0,1)\n",
    "x_b0_np = np.zeros((N_b//2,1))\n",
    "x_b1_np = np.ones((N_b//2,1))\n",
    "x_c  = tf.convert_to_tensor(x_c_np)\n",
    "x_b0 = tf.convert_to_tensor(x_b0_np)\n",
    "x_b1 = tf.convert_to_tensor(x_b1_np)\n",
    "\n",
    "# -------- Model builder --------\n",
    "class Sin(layers.Layer):\n",
    "    def call(self, z): return tf.sin(z)\n",
    "\n",
    "def build_pinn(activation='tanh', init='glorot_uniform'):\n",
    "    if activation == 'tanh':\n",
    "        act = layers.Activation('tanh')\n",
    "    elif activation == 'sigmoid':\n",
    "        act = layers.Activation('sigmoid')\n",
    "    elif activation == 'sin':\n",
    "        act = Sin()\n",
    "    else:\n",
    "        raise ValueError(\"activation must be 'tanh', 'sigmoid', or 'sin'\")\n",
    "\n",
    "    if init == 'glorot_uniform':\n",
    "        kinit = initializers.GlorotUniform()\n",
    "    elif init == 'he_uniform':\n",
    "        kinit = initializers.HeUniform()\n",
    "    else:\n",
    "        raise ValueError(\"init must be 'glorot_uniform' or 'he_uniform'\")\n",
    "\n",
    "    NN = keras.Sequential([\n",
    "        layers.Input((1,), dtype=tf.float64),\n",
    "        layers.Dense(32, activation=None, kernel_initializer=kinit, bias_initializer='zeros', dtype=tf.float64),\n",
    "        act,\n",
    "        layers.Dense(10, activation=None, kernel_initializer=kinit, bias_initializer='zeros', dtype=tf.float64),\n",
    "        act,\n",
    "        layers.Dense(10, activation=None, kernel_initializer=kinit, bias_initializer='zeros', dtype=tf.float64),\n",
    "        act,\n",
    "        layers.Dense(10, activation=None, kernel_initializer=kinit, bias_initializer='zeros', dtype=tf.float64),\n",
    "        act,\n",
    "        layers.Dense(1,  activation=None, kernel_initializer=kinit, bias_initializer='zeros', dtype=tf.float64),\n",
    "    ])\n",
    "    return NN\n",
    "\n",
    "# -------- PINN losses (PDE + BC) --------\n",
    "@tf.function\n",
    "def pde_residual(model, x):\n",
    "    x = tf.cast(x, tf.float64)\n",
    "    with tf.GradientTape(persistent=True) as t2:\n",
    "        t2.watch(x)\n",
    "        with tf.GradientTape() as t1:\n",
    "            t1.watch(x)\n",
    "            u = model(x)                # (N,1)\n",
    "        u_x = t1.gradient(u, x)         # (N,1)\n",
    "    u_xx = t2.gradient(u_x, x)          # (N,1)\n",
    "    del t2\n",
    "    return v * u_x - k * u_xx           # residual: v u' - k u''\n",
    "\n",
    "def pinn_loss(model, x_c, x_b0, x_b1, lam=1.0):\n",
    "    r = pde_residual(model, x_c)\n",
    "    L_pde = tf.reduce_mean(tf.square(r))\n",
    "    u_b0 = model(x_b0)\n",
    "    u_b1 = model(x_b1)\n",
    "    L_bc = tf.reduce_mean(tf.square(u_b0)) + tf.reduce_mean(tf.square(u_b1 - 1.0))\n",
    "    return (L_pde / lam) + L_bc, L_pde, L_bc\n",
    "\n",
    "# -------- One training run --------\n",
    "def train_one(model, steps=20000, lr=5e-3, lam=1.0, patience=500, min_lr=1e-6, verbose=False):\n",
    "    opt = keras.optimizers.Adam(learning_rate=lr)\n",
    "    best = np.inf\n",
    "    plateau = 0\n",
    "    for it in range(steps):\n",
    "        with tf.GradientTape() as tape:\n",
    "            loss, Lp, Lb = pinn_loss(model, x_c, x_b0, x_b1, lam=lam)\n",
    "        grads = tape.gradient(loss, model.trainable_variables)\n",
    "        opt.apply_gradients(zip(grads, model.trainable_variables))\n",
    "\n",
    "        # reduce-on-plateau every 200 iters\n",
    "        if (it+1) % 200 == 0:\n",
    "            cur = float(loss.numpy())\n",
    "            if verbose: print(f\"it {it+1:5d}  loss={cur:.3e}  Lp={float(Lp.numpy()):.3e}  Lb={float(Lb.numpy()):.3e}  lr={opt.learning_rate.numpy():.2e}\")\n",
    "            if cur < best - 1e-9:\n",
    "                best = cur; plateau = 0\n",
    "            else:\n",
    "                plateau += 1\n",
    "                if plateau >= patience//200 and opt.learning_rate.numpy() > min_lr + 1e-12:\n",
    "                    opt.learning_rate.assign(max(opt.learning_rate.numpy()*0.5, min_lr))\n",
    "                    plateau = 0\n",
    "\n",
    "    # MSE on dense grid\n",
    "    u_hat = model(tf.convert_to_tensor(x_eval))\n",
    "    mse = float(tf.reduce_mean(tf.square(u_hat - u_eval)).numpy())\n",
    "    return mse\n",
    "\n",
    "# -------- Run multiple seeds per config --------\n",
    "def set_seed(s):\n",
    "    random.seed(s); np.random.seed(s); tf.random.set_seed(s)\n",
    "\n",
    "def run_config(label, activation, init, n_runs=20, steps=20000, lr=5e-3, lam=1.0):\n",
    "    mses = []\n",
    "    for i in range(n_runs):\n",
    "        set_seed(1000 + i)\n",
    "        model = build_pinn(activation=activation, init=init)\n",
    "        mse = train_one(model, steps=steps, lr=lr, lam=lam)\n",
    "        mses.append(mse)\n",
    "    print(f\"{label}: median MSE = {np.median(mses):.3e}\")\n",
    "    return mses\n",
    "\n",
    "# -------- Define configurations (as in Fig. 5(a) for PINNs) --------\n",
    "configs = [\n",
    "    (\"Std-Xav-Tanh\",    'tanh',    'glorot_uniform'),\n",
    "    (\"Std-He-Tanh\",     'tanh',    'he_uniform'),\n",
    "    (\"Std-Xav-Sigmoid\", 'sigmoid', 'glorot_uniform'),\n",
    "    (\"Std-He-Sigmoid\",  'sigmoid', 'he_uniform'),\n",
    "    (\"Std-Xav-Sin\",     'sin',     'glorot_uniform'),\n",
    "    (\"Std-He-Sin\",      'sin',     'he_uniform'),\n",
    "]\n",
    "\n",
    "labels, data = [], []\n",
    "for name, act, init in configs:\n",
    "    mses = run_config(name, act, init, n_runs=20, steps=50000, lr=5e-3, lam=1.0)\n",
    "    labels.append(name); data.append(mses)\n",
    "\n",
    "# -------- Boxplot (log scale) --------\n",
    "plt.figure(figsize=(10,4))\n",
    "plt.boxplot(data, showfliers=False)\n",
    "plt.yscale('log')\n",
    "plt.xticks(range(1, len(labels)+1), labels, rotation=45, ha='right')\n",
    "plt.ylabel('Solution MSE (log scale)')\n",
    "plt.title(f'1D steady convectionâ€“diffusion (v={v:g}, k={k:g}): PINN MSE across 20 runs')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "plt.savefig(\"fig5a_pinns.png\", dpi=300, bbox_inches='tight')\n",
    "#plt.close()\n",
    "\n",
    "import pickle\n",
    "\n",
    "# Save\n",
    "with open(\"results.pkl\", \"wb\") as f:\n",
    "    pickle.dump({\"labels\": labels, \"data\": data}, f)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
